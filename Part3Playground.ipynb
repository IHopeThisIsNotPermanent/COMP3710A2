{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the pytorch example for CIFAR10\n",
    "\n",
    "#### Note on downloading the dataset:\n",
    "Downloaded the cifar10 dataset manually using curl, but the code below will download it if it missing from the datasets folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "d = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./datasets', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./datasets', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, padding = 1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.pool4 = nn.MaxPool2d(4, 4)\n",
    "        self.fc1 = nn.Linear(128, 100)\n",
    "        self.fc2 = nn.Linear(100, 60)\n",
    "        self.fc3 = nn.Linear(60, 40)\n",
    "        self.fc4 = nn.Linear(40, 20)\n",
    "        self.fc5 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool2(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool2(F.relu(self.conv3(x)))\n",
    "        x = self.pool4(F.relu(self.conv4(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.304\n",
      "[1,  4000] loss: 2.303\n",
      "[1,  6000] loss: 2.301\n",
      "[1,  8000] loss: 2.262\n",
      "[1, 10000] loss: 2.104\n",
      "[1, 12000] loss: 1.998\n",
      "[2,  2000] loss: 1.904\n",
      "[2,  4000] loss: 1.846\n",
      "[2,  6000] loss: 1.747\n",
      "[2,  8000] loss: 1.677\n",
      "[2, 10000] loss: 1.614\n",
      "[2, 12000] loss: 1.569\n",
      "[3,  2000] loss: 1.505\n",
      "[3,  4000] loss: 1.449\n",
      "[3,  6000] loss: 1.425\n",
      "[3,  8000] loss: 1.369\n",
      "[3, 10000] loss: 1.338\n",
      "[3, 12000] loss: 1.271\n",
      "[4,  2000] loss: 1.206\n",
      "[4,  4000] loss: 1.171\n",
      "[4,  6000] loss: 1.166\n",
      "[4,  8000] loss: 1.132\n",
      "[4, 10000] loss: 1.130\n",
      "[4, 12000] loss: 1.077\n",
      "[5,  2000] loss: 1.023\n",
      "[5,  4000] loss: 0.987\n",
      "[5,  6000] loss: 0.988\n",
      "[5,  8000] loss: 0.958\n",
      "[5, 10000] loss: 0.969\n",
      "[5, 12000] loss: 0.970\n",
      "[6,  2000] loss: 0.885\n",
      "[6,  4000] loss: 0.879\n",
      "[6,  6000] loss: 0.881\n",
      "[6,  8000] loss: 0.890\n",
      "[6, 10000] loss: 0.877\n",
      "[6, 12000] loss: 0.863\n",
      "[7,  2000] loss: 0.795\n",
      "[7,  4000] loss: 0.790\n",
      "[7,  6000] loss: 0.815\n",
      "[7,  8000] loss: 0.807\n",
      "[7, 10000] loss: 0.796\n",
      "[7, 12000] loss: 0.812\n",
      "[8,  2000] loss: 0.729\n",
      "[8,  4000] loss: 0.735\n",
      "[8,  6000] loss: 0.729\n",
      "[8,  8000] loss: 0.738\n",
      "[8, 10000] loss: 0.751\n",
      "[8, 12000] loss: 0.739\n",
      "[9,  2000] loss: 0.660\n",
      "[9,  4000] loss: 0.645\n",
      "[9,  6000] loss: 0.689\n",
      "[9,  8000] loss: 0.680\n",
      "[9, 10000] loss: 0.684\n",
      "[9, 12000] loss: 0.685\n",
      "[10,  2000] loss: 0.613\n",
      "[10,  4000] loss: 0.618\n",
      "[10,  6000] loss: 0.638\n",
      "[10,  8000] loss: 0.650\n",
      "[10, 10000] loss: 0.641\n",
      "[10, 12000] loss: 0.636\n",
      "[11,  2000] loss: 0.587\n",
      "[11,  4000] loss: 0.570\n",
      "[11,  6000] loss: 0.574\n",
      "[11,  8000] loss: 0.577\n",
      "[11, 10000] loss: 0.600\n",
      "[11, 12000] loss: 0.613\n",
      "[12,  2000] loss: 0.514\n",
      "[12,  4000] loss: 0.526\n",
      "[12,  6000] loss: 0.550\n",
      "[12,  8000] loss: 0.576\n",
      "[12, 10000] loss: 0.573\n",
      "[12, 12000] loss: 0.575\n",
      "[13,  2000] loss: 0.486\n",
      "[13,  4000] loss: 0.508\n",
      "[13,  6000] loss: 0.519\n",
      "[13,  8000] loss: 0.523\n",
      "[13, 10000] loss: 0.535\n",
      "[13, 12000] loss: 0.543\n",
      "[14,  2000] loss: 0.447\n",
      "[14,  4000] loss: 0.499\n",
      "[14,  6000] loss: 0.485\n",
      "[14,  8000] loss: 0.491\n",
      "[14, 10000] loss: 0.511\n",
      "[14, 12000] loss: 0.509\n",
      "[15,  2000] loss: 0.429\n",
      "[15,  4000] loss: 0.453\n",
      "[15,  6000] loss: 0.450\n",
      "[15,  8000] loss: 0.474\n",
      "[15, 10000] loss: 0.482\n",
      "[15, 12000] loss: 0.502\n",
      "[16,  2000] loss: 0.413\n",
      "[16,  4000] loss: 0.428\n",
      "[16,  6000] loss: 0.448\n",
      "[16,  8000] loss: 0.449\n",
      "[16, 10000] loss: 0.457\n",
      "[16, 12000] loss: 0.462\n",
      "[17,  2000] loss: 0.387\n",
      "[17,  4000] loss: 0.392\n",
      "[17,  6000] loss: 0.415\n",
      "[17,  8000] loss: 0.444\n",
      "[17, 10000] loss: 0.445\n",
      "[17, 12000] loss: 0.441\n",
      "[18,  2000] loss: 0.369\n",
      "[18,  4000] loss: 0.389\n",
      "[18,  6000] loss: 0.407\n",
      "[18,  8000] loss: 0.430\n",
      "[18, 10000] loss: 0.416\n",
      "[18, 12000] loss: 0.435\n",
      "[19,  2000] loss: 0.362\n",
      "[19,  4000] loss: 0.364\n",
      "[19,  6000] loss: 0.379\n",
      "[19,  8000] loss: 0.383\n",
      "[19, 10000] loss: 0.396\n",
      "[19, 12000] loss: 0.420\n",
      "[20,  2000] loss: 0.330\n",
      "[20,  4000] loss: 0.354\n",
      "[20,  6000] loss: 0.361\n",
      "[20,  8000] loss: 0.374\n",
      "[20, 10000] loss: 0.383\n",
      "[20, 12000] loss: 0.393\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(d), data[1].to(d)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs).to(d)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  \n",
    "            loss_list.append(running_loss / 2000)\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 73 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failed experiments\n",
    "\n",
    "#### Experiment 1\n",
    "\n",
    "```python\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(32, 64, 3, padding = 1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.pool4 = nn.MaxPool2d(4, 4)\n",
    "        self.fc1 = nn.Linear(64, 30)\n",
    "        self.fc2 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool2(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool2(F.relu(self.conv3(x)))\n",
    "        x = self.pool4(F.relu(self.conv4(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "```\n",
    "After 10 epochs got to 65% on test. 16mins on cpu.\n",
    "After 20 epochs got to 66% on test. 23mins on cpu.\n",
    "\n",
    "#### Test 2\n",
    "\n",
    "I made it bigger, and got no decrease in computation speed, but now in 10 epochs i got 73%! the loss began platouing at the end anyway, so I dont think more iterations will increase the test test by that much.\n",
    "```python\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, padding = 1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.pool4 = nn.MaxPool2d(4, 4)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool2(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool2(F.relu(self.conv3(x)))\n",
    "        x = self.pool4(F.relu(self.conv4(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "```\n",
    "\n",
    "#### Test 3\n",
    "\n",
    "I Added more linear layers at the end, and ran it for 20 epochs. Interestingly this did nothing whatsoever, still got a test accuracy of 73%, thought this time ~30 minutes on cpu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
